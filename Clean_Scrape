import scrapy
from scrapy.crawler import CrawlerProcess
import re

class Spidey(scrapy.Spider):
    name = "Spidey"

    def start_requests(self):
        url = 'https://en.wikipedia.org/wiki/Kobe_Bryant'
        yield scrapy.Request(url=url, callback=self.parse_one)

    def parse_one(self, response):
        # Extracting data from the webpage
        quote_headers = response.xpath('//*[@id="mw-content-text"]/div[1]/p[2]').get()

        # Cleaning the extracted data
        cleaned_quote_headers = self.clean_text(quote_headers)

        # Storing cleaned data in a dictionary
        item = {'quote_headers': cleaned_quote_headers}

        # Yielding the item to be exported to CSV
        yield item

    def clean_text(self, text):
        # Remove HTML tags
        cleaned_text = re.sub(r'<.*?>', '', text)
        # Remove extra whitespaces
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        return cleaned_text

# Run the Spider and export data to CSV
process = CrawlerProcess(settings={
    'FEED_FORMAT': 'csv',
    'FEED_URI': 'cleaned_scrape.csv'  # Output file for cleaned data
})
process.crawl(Spidey)
process.start()
