import scrapy
from scrapy.crawler import CrawlerProcess

# Define a class for the Spider
class Spidey(scrapy.Spider):
    name = "Spidey"  # Spider's name

    # Method to start the requests
    def start_requests(self):
        url = 'INSERT_URL_HERE'  # URL to scrape, replace with actual URL
        # Yielding a request with the specified URL and specifying the callback function to handle the response
        yield scrapy.Request(url=url, callback=self.parse_one)

    # Callback function to parse the response
    def parse_one(self, response):
        # Extracting data from the webpage using XPath notation, replace XPATH_NOTATION_HERE with the actual XPath
        quote_header = response.xpath('XPATH_NOTATION_HERE').getall()

        # Storing extracted data in a dictionary
        item = {'quote_header': quote_header}

        # Yielding the item to be exported to CSV
        yield item


# Run the Spider and export data to CSV
process = CrawlerProcess(settings={
    'FEED_FORMAT': 'csv',  # Setting the output format to CSV
    'FEED_URI': 'first_scrape.csv'  # Specifying the filename for the CSV output
})
process.crawl(Spidey)  # Run the spider
process.start()  # Start the crawler process
